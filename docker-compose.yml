# ==============================================================================
# Docker Compose para API de Clonagem de Voz (Coqui TTS)
# ==============================================================================

services:
  # Serviço CPU - Ideal para desenvolvimento e ambientes sem GPU
  tts-api-cpu:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: eopix-tts-api-cpu
    image: eopix/tts-api:latest-cpu
    ports:
      - "8000:8000"
    volumes:
      # Montar diretórios de áudio para persistência
      - ./audio:/app/audio:rw
      - ./api/audio/uploads:/app/api/audio/uploads:rw
      - ./api/audio/outputs:/app/api/audio/outputs:rw
      # Cache de modelos (evita re-download) - usando /home/appuser
      - tts-models-cache:/home/appuser/.local/share/tts
      - huggingface-cache:/home/appuser/.cache/huggingface
    environment:
      - PYTHONUNBUFFERED=1
      - TTS_HOME=/home/appuser/.local/share/tts
      - HF_HOME=/home/appuser/.cache/huggingface
      - NUMBA_CACHE_DIR=/tmp/numba_cache
      - COQUI_TOS_AGREED=1
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - eopix-network
    # Limites de recursos (ajuste conforme necessário)
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G

  # Serviço GPU - Para produção com aceleração de hardware
  tts-api-gpu:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    container_name: eopix-tts-api-gpu
    image: eopix/tts-api:latest-gpu
    runtime: nvidia
    ports:
      - "8001:8000"
    volumes:
      # Montar diretórios de áudio para persistência
      - ./audio:/app/audio:rw
      - ./api/audio/uploads:/app/api/audio/uploads:rw
      - ./api/audio/outputs:/app/api/audio/outputs:rw
      # Cache de modelos (evita re-download) - usando /home/appuser
      - tts-models-cache:/home/appuser/.local/share/tts
      - huggingface-cache:/home/appuser/.cache/huggingface
    environment:
      - PYTHONUNBUFFERED=1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - TTS_HOME=/home/appuser/.local/share/tts
      - HF_HOME=/home/appuser/.cache/huggingface
      - NUMBA_CACHE_DIR=/tmp/numba_cache
      - COQUI_TOS_AGREED=1
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    networks:
      - eopix-network
    # Configuração de GPU
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

# ==============================================================================
# Volumes persistentes
# ==============================================================================
volumes:
  tts-models-cache:
    name: eopix-tts-models-cache
  huggingface-cache:
    name: eopix-huggingface-cache

# ==============================================================================
# Rede
# ==============================================================================
networks:
  eopix-network:
    name: eopix-network
    driver: bridge
